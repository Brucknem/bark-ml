{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adverserial Imitation Learning\n",
    "\n",
    "Generative Adversarial Imitation Learning (GAIL) was first proposed in the paper [Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476) by Jonathan Ho and Stefano Ermon. The project task is to implement the Generative Adverserial Imitation Learning model for driving scenarios using the BARK-simulator.\n",
    "\n",
    "GAIL is based on the setting of Reinforcement Learning (RL). In Reinforcement Learning, the agent interacts with the environment through its actions and receives rewards in return. The aim of the learning process is to maximize the cummulative reward by chosing the best action in all states.\n",
    "\n",
    "As the name suggests, GAIL belongs to a smaller subgroup of RL, called Imitation Learning. In this setup the goal of the agent is to mimic an expert behavior as closely as possible. The environment awards higher rewards to expert-like behavior and smaller ones to actions which substantially deviate from the expert behavior. In our case, expert trajectories were generated from real life data, namely from the Interaction Dataset, as well as from a pretrained SAC (Soft Actor-Critic) agent. The expert trajectories, which are obtained in this way, represent the expert knowledge by containing many states with corresponding actions that were produced by the expert.\n",
    "\n",
    "As mentioned previously, learning of the agent in the RL setting is driven by the rewards it receives from the environment. The amount of the reward in the Imitation Learning setting are determined based on how closely the agent mimics the expert behavior. Special in the GAIL approach is that we receive the reward from an adversarial game: The agent is represented by a generator network which is trained based on the feedback of a discriminator network. The generator produces actions for given states which are then evaluated by the discriminator. In the meantime, the discriminator is trained by feeding it with expert and agent state-action pairs for classification. This way, the generator tries to fool the discriminator, hence he aims at acting as expert-like as possible. Meanwhile, the discriminator tries to distinguish between expert and agent trajectories. Intuitively, learning converges when the generator learned to act so similarly to the expert that the discriminator cannot tell apart expert and agent trajectories any more. In game theory this point is called the Nash-equilibrium.\n",
    "\n",
    "In practice, the implementation of a GAIL agent is usually solved in the following way for sample efficiency: The agent interacts with the environment by following its actual policy and hence agent state-action pairs are generated. These points are stored in a replay buffer for further learning. After a specified interval a training step is carried out. This training step has 2 substeps: training the discriminator and training the generator networks. \n",
    "* __Discriminator training:__ The discriminator is fed a batch of expert (from the expert trajectories) and agent (from the replay buffer) state-action pairs. It classifies all of them. Based on their true labels the loss is calculated and a gradient descent step is carried out in order to minimize the loss.\n",
    "* __Generator training:__ The generator is fed a batch of states from the replay buffer and it produces actions for them. The resulting state-action pairs are fed to the discriminator for classification. The negative output of the discriminator is used as a loss for the generator network. (Close to -1 if the agent mimics the expert successfully.) The gradient of the loss is propagated all the way back to the generator network to carry out a gradient step to minimize its loss.<br>\n",
    "As already stated, training runs until both, the generator and the discriminator loss, converge to a steady state value.\n",
    "\n",
    "\n",
    "The training process is visualized in the following figure:\n",
    "\n",
    "\n",
    "<img width=70% src=\"files/data/gail_overview.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Interaction Dataset\n",
    "As data source, we use the Interaction Dataset: https://arxiv.org/abs/1910.03088. We are interested in the merging scenarios: \n",
    "* DR_DEU_Merging_MT\n",
    "* DR_CHN_Merging_ZS\n",
    "\n",
    "These scenarios contain a map specification and track specifications for multiple vehicles that drive on the map. The tracks represent the trajectories of the vehicles which consist of a number of consecutive recorded states. \n",
    "\n",
    "Have a look how the Interaction Dataset is [integrated in BARK](https://github.com/bark-simulator/bark/blob/setup_tutorials/docs/tutorials/04_interaction_dataset.ipynb). \n",
    "(Note that the dataset itself is NOT enclosed within BARK due to license limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Trajectories\n",
    "\n",
    "The Interaction Dataset of course contains trajectories of many different vehicles with different wheel bases. As there are only states recorded in the dataset, we calculate the action the vehicle has taken to go from one state to the next ourselves.\n",
    "\n",
    "The wheel base is used to calculate the action following the [Single Track Model](https://borrelli.me.berkeley.edu/pdfpub/IV_KinematicMPC_jason.pdf). As we are only interested in trajectories with a wheel base matching our agent, we use a fixed wheel base when calculating the actions from the successive states. This however does not restrict the accuracy of the data. We just consider that all state trajectories were carried out by the same car and calculate the actions that a car would have needed to carry out that behavior. \n",
    "\n",
    "As the state variables and also the actions have different magnitudes, we normalize all of them for training. The normalization of the expert trajectories takes place while loading the generated expert trajectories. The loading function also takes the current environment (BARK runtime) as an input, hence the trajectories are normalized according to the current parameters (Current state and action spaces).\n",
    "\n",
    "You can have a look at the source code in `bark_ml.library_wrappers.lib_tf2rl.generate_expert_trajectories`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Expert Trajectories\n",
    "A short example script for generating expert trajectories from the interaction dataset is shown in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import bark\n",
    "from pprint import pprint\n",
    "from bark_ml.library_wrappers.lib_tf2rl.generate_expert_trajectories import *\n",
    "\n",
    "tracks_folder = os.path.join(os.getcwd(), 'data')\n",
    "map_file = os.path.join(os.getcwd(), 'data/DR_DEU_Merging_MT_v01_shifted.xodr')\n",
    "known_key = ('DR_DEU_Merging_MT_v01_shifted', 'vehicle_tracks_013')\n",
    "ego_agent = 66\n",
    "\n",
    "param_server = create_parameter_servers_for_scenarios(map_file, tracks_folder)[known_key]\n",
    "generation_params = param_server[\"Scenario\"][\"Generation\"][\"InteractionDatasetScenarioGeneration\"]\n",
    "generation_params[\"TrackIds\"] = [63, 64, 65, 66, 67, 68]\n",
    "generation_params[\"StartTs\"] = 232000\n",
    "generation_params[\"EndTs\"] = 259000\n",
    "generation_params[\"EgoTrackId\"] = ego_agent\n",
    "param_server[\"Scenario\"][\"Generation\"][\"InteractionDatasetScenarioGeneration\"] = generation_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expert_trajectories = generate_expert_trajectories_for_scenario(param_server, sim_time_step=200, renderer=\"matplotlib_jupyter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generated expert trajectories\n",
    "The generated expert trajectories are stored in a dictionary with key-value pairs:\n",
    "* `obs`: list, contains the observation vector for the timestep.\n",
    "* `act`: list, contains the action that was carried out in that timestep.\n",
    "* `next_obs`: list, contains the next observation after carrying out the action `act` in the state `obs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of observations\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\Theta \\\\\n",
    "v\n",
    "\\end{pmatrix}\n",
    "\\text{ for the ego vehicle (0) and the three nearest vehicles (1..3) in the scene, where $x$ and $y$ are 2D coordinates, $\\Theta$ is orientation and $v$ velocity.}\n",
    "\\end{align*}\n",
    "\n",
    "Stored as `expert_trajectories[ego_agent]['obs']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small number of observations for our agent\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "display(observations_to_dataframe(expert_trajectories[ego_agent]['obs'][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of actions\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "a \\\\\n",
    "\\delta \n",
    "\\end{pmatrix}\n",
    "\\text{, where $a$ is acceleration and $\\delta$ is steering angle.}\n",
    "\\end{align*}\n",
    "\n",
    "Stored as `expert_trajectories[ego_agent]['act']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Small number of actions from our agent\n",
    "\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "display(actions_to_dataframe(expert_trajectories[ego_agent]['act'][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# GAIL implementation\n",
    "The following section describes the implementation details of our Generative Adverserial Imitation Learning setup.\n",
    "\n",
    "## TF2RL implementation\n",
    "We use an off the shelf implementation, the library [TF2RL](https://github.com/keiohta/tf2rl). It implements several reinforcement learning algorithms in [tensorflow 2](https://www.tensorflow.org/guide/effective_tf2). \n",
    "\n",
    "The GAIL agent is built up as follows:\n",
    "* __Generator:__ A complete DDPG agent with actor and critic networks. Both of them have 2-2 hidden layers.\n",
    "* __Discriminator:__ A normal discriminator network with 2 hidden layers.\n",
    "\n",
    "In this respect, the agent is not in the traditional GAIL setup with 2 neural networks. Instead, it has 5 networks, since the DDPG agent itself has 4 networks for greater stability during training. The DDPG agent's critic network receives the judgement of the discriminator network as the reward from the environment and its training aims to maximize this reward.\n",
    "\n",
    "## Integration into BARK\n",
    "The TF2RL based GAIL agent is integrated into the existing BARK concepts and is implemented in the following most important classes:\n",
    "* __TF2RLWrapper:__ Wraps the BARK runtime to match the expectations of TF2RL about the environment. The observation and action normalization also takes place here.\n",
    "    * Source: `bark_ml/library_wrappers/lib_tf2rl/tf2rl_wrapper.py`\n",
    "* __BehaviorTF2RLAgent:__ Base class for TF2RL based agents.\n",
    "    * Source: `bark_ml/library_wrappers/lib_tf2rl/agents/tf2rl_agent.py`\n",
    "* __BehaviorGAILAgent:__ The TF2RL based GAIL agent.\n",
    "    * Source: `bark_ml/library_wrappers/lib_tf2rl/agents/gail_agent.py`\n",
    "* __TF2RLRunner:__ Base class for TF2RL based runners.\n",
    "    * Source: `bark_ml/library_wrappers/lib_tf2rl/runners/tf2rl_runner.py`\n",
    "* __GAILRunner:__ The TF2RL based GAIL runner.\n",
    "    * Source: `bark_ml/library_wrappers/lib_tf2rl/runners/gail_runner.py`\n",
    "    \n",
    "In the following, the training process is demonstrated. Later the performance of a pre-trained agent can be visualized.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We will now train a GAIL agent using the implementation described above. There are several training parameters which can be set on demand:\n",
    "* The number of steps to train for\n",
    "* The frequency of testing during training\n",
    "* The number of episodes in each testing round\n",
    "* The usage of GPU accelerated calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize some parameters here!\n",
    "\n",
    "max_steps = 100000          # Number of steps to train for.\n",
    "test_interval = 100         # test in every ... steps.\n",
    "test_episodes = 5           # number of test episodes.\n",
    "gpu = 0                     # use -1 for cpu only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# BARK imports\n",
    "from bark_project.bark.runtime.commons.parameters import ParameterServer\n",
    "from bark.runtime.viewer.matplotlib_viewer import MPViewer\n",
    "from bark.runtime.viewer.video_renderer import VideoRenderer\n",
    "\n",
    "# BARK-ML imports\n",
    "from bark_ml.environments.blueprints import ContinuousHighwayBlueprint, \\\n",
    "  ContinuousMergingBlueprint, ContinuousIntersectionBlueprint\n",
    "from bark_ml.environments.single_agent_runtime import SingleAgentRuntime\n",
    "from bark_ml.library_wrappers.lib_tf2rl.tf2rl_wrapper import TF2RLWrapper\n",
    "from bark_ml.library_wrappers.lib_tf2rl.agents.gail_agent import BehaviorGAILAgent\n",
    "from bark_ml.library_wrappers.lib_tf2rl.runners.gail_runner import GAILRunner\n",
    "from bark_ml.library_wrappers.lib_tf2rl.load_expert_trajectories import load_expert_trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training graphs\n",
    "\n",
    "The next cell deletes the previous logs and launches tensorboard. After tensorboard has launched, please go on to the next cell and start the training. The tensorboard window refreshes itself every 30 seconds, but you can also refresh it manually in the upper right corner.\n",
    "\n",
    "You should see the graphs for:\n",
    "* **Common**\n",
    "    * Common/average_step_count: The average number of steps the agent takes in the environment per scenario\n",
    "    * Common/average_test_return: The average return during the test scenarios\n",
    "    * Common/fps: The steps the agent takes in the environment per second\n",
    "    * Common/training_return: The per scenario return of the agent during training\n",
    "* **DDPG**\n",
    "    * DDPG/actor_loss: The loss of the actor network\n",
    "    * DDPG/critic_loss: The loss of the critic network\n",
    "* **GAIL**\n",
    "    * GAIL/Accuracy: The agent/expert distinguishing accuracy of the discriminator\n",
    "    * GAIL/DiscriminatorLoss: The loss of the discriminator network\n",
    "    * GAIL/JSdivergence: The Jensen–Shannon divergence measuring the similarity between the expert and agent\n",
    "\n",
    "The GAIL agent should converge to a Common/average_test_return of 1, so success in every scenario it faces, after at most 10.000 scenarios. If it doesn't converge, then the initial network weights were not well drawn. This is a common problem in the Generative Adverserial setup. In this case, please rerun the training, as the network weights are then newly initialized at random.\n",
    "\n",
    "***\n",
    "\n",
    "Sometimes tensorboard does not refresh correctly. If you don't see all of the above graphs after 300 scenarios, please rightclick the tensorboard and click _Reload frame_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# launching tensorboard and deleting the previous runs logdirs:\n",
    "%rm -r \"data/logs\"\n",
    "%mkdir \"data/logs\"\n",
    "%tensorboard --logdir \"data/logs\" --port=60060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load params from the json file to create the parameter server object\n",
    "params = ParameterServer(filename=\"data/params/gail_params.json\")\n",
    "\n",
    "# customized parameters:\n",
    "params[\"ML\"][\"Settings\"][\"GPUUse\"] = gpu\n",
    "tf2rl_params = params[\"ML\"][\"GAILRunner\"][\"tf2rl\"]\n",
    "tf2rl_params[\"max_steps\"] = max_steps\n",
    "tf2rl_params[\"test_interval\"] = test_interval\n",
    "tf2rl_params[\"test_episodes\"] = test_episodes\n",
    "params[\"ML\"][\"GAILRunner\"][\"tf2rl\"] = tf2rl_params\n",
    "if params[\"ML\"][\"BehaviorGAILAgent\"][\"WarmUp\"] > max_steps / 2:\n",
    "    params[\"ML\"][\"BehaviorGAILAgent\"][\"WarmUp\"] = max_steps / 2\n",
    "\n",
    "# create environment\n",
    "bp = ContinuousMergingBlueprint(params,\n",
    "                                number_of_senarios=500,\n",
    "                                random_seed=0)\n",
    "env = SingleAgentRuntime(blueprint=bp,\n",
    "                         render=False)\n",
    "\n",
    "# wrapped environment for compatibility with tf2rl\n",
    "wrapped_env = TF2RLWrapper(env, \n",
    "                           normalize_features=params[\"ML\"][\"Settings\"][\"NormalizeFeatures\"])\n",
    "\n",
    "# instantiate the GAIL agent\n",
    "gail_agent = BehaviorGAILAgent(environment=wrapped_env,\n",
    "                               params=params)\n",
    "\n",
    "# load the expert trajectories\n",
    "expert_trajectories, _, _ = load_expert_trajectories(\n",
    "    params['ML']['ExpertTrajectories']['expert_path_dir'],\n",
    "    normalize_features=params[\"ML\"][\"Settings\"][\"NormalizeFeatures\"],\n",
    "    env=env, # the unwrapped env has to be used, since that contains the unnormalized spaces.\n",
    "    subset_size=params[\"ML\"][\"ExpertTrajectories\"][\"subset_size\"]\n",
    "    ) \n",
    "\n",
    "# instantiate a runner that is going to train the agent\n",
    "runner = GAILRunner(params=params,\n",
    "                 environment=wrapped_env,\n",
    "                 agent=gail_agent,\n",
    "                 expert_trajs=expert_trajectories)\n",
    "\n",
    "# train the agent\n",
    "runner.Train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualization of a trained agent \n",
    "\n",
    "Lastly, we show you how a trained agent interacts with the environment.\n",
    "Therefore, please set up the number of scenarios to visualize in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of scenarios to visualize\n",
    "num_scenarios_to_visualize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load params from the json file to create the parameter server object\n",
    "params = ParameterServer(filename=\"data/params/gail_params.json\")\n",
    "\n",
    "# setting the path for the pretrained agent.\n",
    "params[\"ML\"][\"GAILRunner\"][\"tf2rl\"][\"model_dir\"] = \"../../../com_github_gail_4_bark_large_data_store/pretrained_agents/gail/merging\"\n",
    "\n",
    "# customized parameters:\n",
    "params[\"ML\"][\"Settings\"][\"GPUUse\"] = gpu\n",
    "tf2rl_params = params[\"ML\"][\"GAILRunner\"][\"tf2rl\"]\n",
    "tf2rl_params[\"max_steps\"] = max_steps\n",
    "tf2rl_params[\"test_interval\"] = test_interval\n",
    "tf2rl_params[\"test_episodes\"] = test_episodes\n",
    "params[\"ML\"][\"GAILRunner\"][\"tf2rl\"] = tf2rl_params\n",
    "if params[\"ML\"][\"BehaviorGAILAgent\"][\"WarmUp\"] > max_steps / 2:\n",
    "    params[\"ML\"][\"BehaviorGAILAgent\"][\"WarmUp\"] = max_steps / 2\n",
    "\n",
    "# create environment\n",
    "bp = ContinuousMergingBlueprint(params,\n",
    "                              number_of_senarios=500,\n",
    "                              random_seed=0)\n",
    "env = SingleAgentRuntime(blueprint=bp,\n",
    "                      render=False)\n",
    "\n",
    "# wrapped environment for compatibility with tf2rl\n",
    "wrapped_env = TF2RLWrapper(env, \n",
    "normalize_features=params[\"ML\"][\"Settings\"][\"NormalizeFeatures\"])\n",
    "\n",
    "# instantiate the GAIL agent\n",
    "gail_agent = BehaviorGAILAgent(environment=wrapped_env,\n",
    "                           params=params)\n",
    "\n",
    "# instantiate a runner that is going to train the agent.\n",
    "runner = GAILRunner(params=params,\n",
    "                 environment=wrapped_env,\n",
    "                 agent=gail_agent,)\n",
    "\n",
    "# Visualize the agent\n",
    "runner.Visualize(num_scenarios_to_visualize, renderer=\"matplotlib_jupyter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
